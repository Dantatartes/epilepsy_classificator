{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4lmA0PAuuvXA"
   },
   "outputs": [],
   "source": [
    "from model import *\n",
    "from data_preprocessing import *\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "86vaZYOeuvXK"
   },
   "outputs": [],
   "source": [
    "def test_on_folds(model):\n",
    "    with open('folds.pkl', 'rb') as f:\n",
    "        folds = pickle.load(f)\n",
    "    \n",
    "    accuracy = 0\n",
    "    for train, test in zip(\n",
    "        [folds[:i] + folds[i + 1:] for i in range(len(folds))],\n",
    "        folds\n",
    "    ):\n",
    "        train = functools.reduce(lambda x, y: x + y, train)\n",
    "        model.train(train)\n",
    "        accuracy += model.predict_test(test)\n",
    "        print(accuracy)\n",
    "    \n",
    "    print(f'Accuracy: {accuracy / 5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k9-_akqNHYDM"
   },
   "outputs": [],
   "source": [
    "def test_on_test_train(model):\n",
    "    with open('folds.pkl', 'rb') as f:\n",
    "        folds = pickle.load(f)\n",
    "    \n",
    "    accuracy = 0\n",
    "    for train, test in zip(\n",
    "        [folds[:i] + folds[i + 1:] for i in range(len(folds))],\n",
    "        folds\n",
    "    ):\n",
    "        train = functools.reduce(lambda x, y: x + y, train)\n",
    "        model.train(train)\n",
    "        accuracy += model.predict_test(test)\n",
    "        print(accuracy)\n",
    "    \n",
    "    print(f'Accuracy: {accuracy / 5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uvRmPFpSuvXS"
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    labels = {\n",
    "        'ABSZ': 30, \n",
    "        'CPSZ': 8, \n",
    "        'FNSZ': 3, \n",
    "        'GNSZ': 7, \n",
    "        'SPSZ': 67, \n",
    "        'TCSZ': 60, \n",
    "        'TNSZ': 44\n",
    "    }\n",
    "    \n",
    "    def __init__(self, data_dir, model, sklearn_=False, pytorch_=True, is_cuda=False):\n",
    "        self.dir = data_dir\n",
    "        self.model = model\n",
    "        self.is_cuda = is_cuda\n",
    "        self.have_label = True\n",
    "        if pytorch_:\n",
    "            self.pytorch = True\n",
    "            self.sklearn = False\n",
    "        else:\n",
    "            self.pytorch = False\n",
    "            self.sklearn = True\n",
    "            \n",
    "    def read_data(self, files, dir_new, df=False):\n",
    "        cnt = 0\n",
    "        if os.path.isdir(dir_new):\n",
    "            shutil.rmtree(dir_new)\n",
    "            os.mkdir(dir_new)\n",
    "        else:\n",
    "            os.mkdir(dir_new)\n",
    "            \n",
    "        if df:\n",
    "            df_array_flag = True\n",
    "            lst_y = []\n",
    "        \n",
    "        for file in tqdm_notebook(files):\n",
    "            with open(f'{self.dir}/{file}', 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                data, label = data[1], data[0]\n",
    "                if label not in self.labels.keys():\n",
    "                    continue\n",
    "                \n",
    "                np.random.shuffle(data)\n",
    "                while len(data) < self.labels[label]:\n",
    "                    data = np.concatenate([data, data])\n",
    "                \n",
    "                data = data[:self.labels[label]]\n",
    "                \n",
    "                if df and df_array_flag:\n",
    "                    df_array_flag = False\n",
    "                    df_array = data\n",
    "                    lst_y.extend([labels_2_num[label]] * self.labels[label])\n",
    "                elif df:\n",
    "                    df_array = np.concatenate([df_array, data])\n",
    "                    lst_y.extend([labels_2_num[label]] * self.labels[label])\n",
    "                \n",
    "                for i in range(len(data)):\n",
    "                    with open(f'{dir_new}/seiz_{cnt}.pkl', 'wb') as f:\n",
    "                        pickle.dump(data_tuple(label, data[i]), f)\n",
    "                        cnt += 1\n",
    "        if df:\n",
    "            df_array = np.concatenate([df_array, np.array(lst_y).reshape(-1, 1)], axis=1)\n",
    "            return pd.DataFrame(df_array, columns=np.arange(901))\n",
    "        \n",
    "    def read_data_stds(self, files, dir_new):\n",
    "        df_array_flag = True\n",
    "        lst_y = []\n",
    "        \n",
    "        for file in tqdm_notebook(files):\n",
    "            with open(f'{self.dir}/{file}', 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                data, label = data[1], data[0]\n",
    "                if label not in self.labels.keys():\n",
    "                    continue\n",
    "                \n",
    "                data = np.array([\n",
    "                    *list(np.std(data, axis=0)),\n",
    "                    *list(np.mean(data, axis=0)),\n",
    "                    *list(np.max(data, axis=0)),\n",
    "                    *list(np.min(data, axis=0)),\n",
    "                ]).reshape(1, -1)\n",
    "                \n",
    "                if df_array_flag:\n",
    "                    df_array_flag = False\n",
    "                    df_array = data\n",
    "                    lst_y.append(labels_2_num[label])\n",
    "                else:\n",
    "                    df_array = np.concatenate([df_array, data], axis=0)\n",
    "                    lst_y.append(labels_2_num[label])\n",
    "        \n",
    "        df_array = np.concatenate([df_array, np.array(lst_y).reshape(-1, 1)], axis=1)\n",
    "        return pd.DataFrame(df_array, columns=np.arange(df_array.shape[1]))\n",
    "                    \n",
    "    \n",
    "    def train(self, train):\n",
    "        print('Starting prepare data...')\n",
    "        if self.sklearn:\n",
    "            self.df = self.read_data_stds(files=train, dir_new=f'{self.dir}_')\n",
    "            # self.df = self.read_data(files=train, dir_new=f'{self.dir}_', df=True)\n",
    "        else:\n",
    "            self.dir_new = f'{self.dir}_'\n",
    "            self.read_data(files=train, dir_new=self.dir_new)\n",
    "        print('Preparing data finished. Starting train model...')\n",
    "        \n",
    "        if self.sklearn:\n",
    "            self.df.iloc[:, :3600] = MinMaxScaler().fit_transform(self.df.iloc[:, :3600])\n",
    "            self.model.fit(self.df.iloc[:, :3600], self.df.iloc[:, 3600])\n",
    "        else:\n",
    "            dataset_ = Dataset(self.dir_new)\n",
    "            self.dataloader_ = data_utils.DataLoader(\n",
    "                dataset=dataset_, \n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "            )\n",
    "            if self.is_cuda:\n",
    "                self.model = self.model.float().cuda()\n",
    "\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            optimizer_ft = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "            exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=17, gamma=0.1)\n",
    "\n",
    "            self.model, losses = train_model(self.model, loss_fn, optimizer_ft, self.dataloader_, is_cuda=self.is_cuda, num_epochs=15)\n",
    "            \n",
    "        print('Model training finished.')\n",
    "    \n",
    "    def predict(self, test, ret_true=False):\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        if self.sklearn:\n",
    "            for file in test:\n",
    "                with open(f'{self.dir}/{file}', 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                y_true.append(labels_2_num[data[0]])\n",
    "                \n",
    "                data = data[1]\n",
    "                np.random.shuffle(data)\n",
    "                array = np.array(data[:128])\n",
    "                pred = self.model.predict(MinMaxScaler().fit_transform(array))\n",
    "                pred = np.array(list(map(int, list(pred))))\n",
    "                counts = np.bincount(pred)\n",
    "                \n",
    "                y_pred.append(np.argmax(counts))\n",
    "        else:\n",
    "            for file in test:\n",
    "                with open(f'{self.dir}/{file}', 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                y_true.append(labels_2_num[data[0]])\n",
    "                \n",
    "                tensor = torch.Tensor(data[1][:128])\n",
    "                if self.is_cuda:\n",
    "                    tensor = tensor.cuda()\n",
    "                pred = self.model(tensor)\n",
    "                pred = pred.std(dim=0)\n",
    "                \n",
    "                y_pred.append(pred.argmax().cpu().tolist())\n",
    "                \n",
    "                \n",
    "        if ret_true:\n",
    "            return y_pred, y_true\n",
    "        else:\n",
    "            return y_pred\n",
    "        \n",
    "    def predict_stds(self, test, ret_true=False):\n",
    "        if self.sklearn:\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            flag = True\n",
    "            \n",
    "            for file in test:\n",
    "                with open(f'{self.dir}/{file}', 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                if self.have_label:\n",
    "                    y_true.append(labels_2_num[data[0]])\n",
    "                else:\n",
    "                    ret_true = False\n",
    "                    ret_letters = True\n",
    "                # data = data[1]                \n",
    "                data = np.array([\n",
    "                    *list(np.std(data, axis=0)),\n",
    "                    *list(np.mean(data, axis=0)),\n",
    "                    *list(np.max(data, axis=0)),\n",
    "                    *list(np.min(data, axis=0)),\n",
    "                ]).reshape(1, -1)\n",
    "                if flag:\n",
    "                    df_array = data\n",
    "                    flag = False\n",
    "                else:\n",
    "                    df_array = np.concatenate([df_array, data])\n",
    "                \n",
    "                    \n",
    "            y_pred = self.model.predict(MinMaxScaler().fit_transform(df_array))\n",
    "        else:\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            \n",
    "            for file in test:\n",
    "                with open(f'{self.dir}/{file}', 'rb') as f:\n",
    "                    data = pickle.load(f)\n",
    "                y_true.append(labels_2_num[data[0]])\n",
    "                \n",
    "                tensor = torch.Tensor(data[1][:128])\n",
    "                if self.is_cuda:\n",
    "                    tensor = tensor.cuda()\n",
    "                pred = self.model(tensor)\n",
    "                pred = pred.std(dim=0)\n",
    "                \n",
    "                y_pred.append(pred.argmax().cpu().tolist())\n",
    "                \n",
    "                \n",
    "        if ret_true:\n",
    "            return y_pred, y_true\n",
    "        elif ret_letters:\n",
    "            y_pred_cls = []\n",
    "            for i, y_ in enumerate(y_pred):\n",
    "                y_pred_cls.append(labels[int(y_)])\n",
    "            return y_pred_cls\n",
    "        return y_pred\n",
    "    \n",
    "    # def predict_test(self, test):\n",
    "    #     pred, true = self.predict_stds(test, ret_true=True)\n",
    "    #     from sklearn.metrics import f1_score\n",
    "    #     return f1_score(true, pred, average='weighted')\n",
    "    \n",
    "    def predict_test(self, test, have_label=False):\n",
    "        self.have_label = have_label\n",
    "        if self.have_label:\n",
    "            pred, true = self.predict_stds(test, ret_true=True)\n",
    "            from sklearn.metrics import f1_score\n",
    "            return f1_score(true, pred, average='weighted')\n",
    "        else:\n",
    "            predict = self.predict_stds(test)\n",
    "            return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQFDGnPBuvXl"
   },
   "outputs": [],
   "source": [
    "  import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prepare data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066bb7847f8d444582aa4e6739847e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1880), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing data finished. Starting train model...\n",
      "Model training finished.\n"
     ]
    }
   ],
   "source": [
    "files_list = [f'test{i}.pkl' for i in range(327)]\n",
    "model = Model(data_dir='./pp2', model=KNeighborsClassifier(n_neighbors=5), pytorch_=False,)\n",
    "model.train([f'seiz_{i}.pkl' for i in range(1, 1881)])\n",
    "prediction = model.predict_test(files_list, have_label=False)\n",
    "files_list = \" \".join(files_list).replace(\".pkl\", \"\")\n",
    "files_list = files_list.split()\n",
    "final_df = pd.DataFrame({'id': files_list, 'label': prediction}, index=None)\n",
    "final_df.to_csv('./predicted.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FNSZ    164\n",
       "GNSZ     73\n",
       "CPSZ     54\n",
       "TNSZ     14\n",
       "SPSZ     11\n",
       "ABSZ      7\n",
       "TCSZ      4\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['label'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "harakiri_v2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
